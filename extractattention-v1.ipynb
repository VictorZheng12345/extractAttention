{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n   # for filename in filenames:\n        #print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(f'device={device}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4head,0extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n        \n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n       \n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        \n        return Vmix0\n    \n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n        \n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n        \n        temp=(temp0+temp1+temp2+temp3)/4\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4head,2extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n\n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        k1=self.extract(k,kweight)\n        attn_weights1 = F.softmax(q @ k1.transpose(-2, -1) / (k1.shape[-1] ** 0.5), dim=-1)\n        Vmix1 = attn_weights1 @ v\n        \n        k2=self.extract(k1,kweight)\n        attn_weights2 = F.softmax(q @ k2.transpose(-2, -1) / (k2.shape[-1] ** 0.5), dim=-1)\n        Vmix2 = attn_weights2 @ v\n        \n        Vmix=(Vmix0+Vmix1+Vmix2)/3\n\n        \n        return Vmix\n    def extract(self,k,kweight):\n        with torch.no_grad():\n            return torch.matmul(torch.matmul(k,kweight),torch.transpose(kweight,0,1))  \n\n\n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n\n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n\n        temp=(temp0+temp1+temp2+temp3)/4\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4head,3extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n\n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        k1=self.extract(k,kweight)\n        attn_weights1 = F.softmax(q @ k1.transpose(-2, -1) / (k1.shape[-1] ** 0.5), dim=-1)\n        Vmix1 = attn_weights1 @ v\n        \n        k2=self.extract(k1,kweight)\n        attn_weights2 = F.softmax(q @ k2.transpose(-2, -1) / (k2.shape[-1] ** 0.5), dim=-1)\n        Vmix2 = attn_weights2 @ v\n        \n        k3=self.extract(k2,kweight)\n        attn_weights3 = F.softmax(q @ k3.transpose(-2, -1) / (k3.shape[-1] ** 0.5), dim=-1)\n        Vmix3 = attn_weights3 @ v\n        \n        Vmix=(Vmix0+Vmix1+Vmix2+Vmix3)/4\n\n        \n        return Vmix\n    def extract(self,k,kweight):\n        with torch.no_grad():\n            return torch.matmul(torch.matmul(k,kweight),torch.transpose(kweight,0,1))  \n            \n\n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n\n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n\n        temp=(temp0+temp1+temp2+temp3)/4\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8head,0extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n        \n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        \n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        \n        \n        return Vmix0\n\n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n        self.Head4=extractAttention( input_dim, output_dim )\n        self.Head5=extractAttention( input_dim, output_dim )\n        self.Head6=extractAttention( input_dim, output_dim )\n        self.Head7=extractAttention( input_dim, output_dim )\n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n        temp4=self.Head4(x)\n        temp5=self.Head5(x)\n        temp6=self.Head6(x)\n        temp7=self.Head7(x)\n        \n        temp=(temp0+temp1+temp2+temp3+temp4+temp5+temp6+temp7)/8\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8head,2extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n        \n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n       \n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        k1=self.extract(k,kweight)\n        attn_weights1 = F.softmax(q @ k1.transpose(-2, -1) / (k1.shape[-1] ** 0.5), dim=-1)\n        Vmix1 = attn_weights1 @ v\n        \n        k2=self.extract(k1,kweight)\n        attn_weights2 = F.softmax(q @ k2.transpose(-2, -1) / (k2.shape[-1] ** 0.5), dim=-1)\n        Vmix2 = attn_weights2 @ v\n        \n        \n        Vmix=(Vmix0+Vmix1+Vmix2)/3\n        \n        \n        return Vmix\n    def extract(self,k,kweight):\n        with torch.no_grad():\n            return torch.matmul(torch.matmul(k,kweight),torch.transpose(kweight,0,1))  \n            \n\n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n        self.Head4=extractAttention( input_dim, output_dim )\n        self.Head5=extractAttention( input_dim, output_dim )\n        self.Head6=extractAttention( input_dim, output_dim )\n        self.Head7=extractAttention( input_dim, output_dim )\n\n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n        temp4=self.Head4(x)\n        temp5=self.Head5(x)\n        temp6=self.Head6(x)\n        temp7=self.Head7(x)\n\n        temp=(temp0+temp1+temp2+temp3+temp4+temp5+temp6+temp7)/8\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8head,3extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n        \n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        \n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        k1=self.extract(k,kweight)\n        attn_weights1 = F.softmax(q @ k1.transpose(-2, -1) / (k1.shape[-1] ** 0.5), dim=-1)\n        Vmix1 = attn_weights1 @ v\n        \n        k2=self.extract(k1,kweight)\n        attn_weights2 = F.softmax(q @ k2.transpose(-2, -1) / (k2.shape[-1] ** 0.5), dim=-1)\n        Vmix2 = attn_weights2 @ v\n        \n        k3=self.extract(k2,kweight)\n        attn_weights3 = F.softmax(q @ k3.transpose(-2, -1) / (k3.shape[-1] ** 0.5), dim=-1)\n        Vmix3 = attn_weights3 @ v\n        \n        Vmix=(Vmix0+Vmix1+Vmix2+Vmix3)/4\n        \n        \n        return Vmix\n    def extract(self,k,kweight):\n        with torch.no_grad():\n            return torch.matmul(torch.matmul(k,kweight),torch.transpose(kweight,0,1))  \n            \n\n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n        self.Head4=extractAttention( input_dim, output_dim )\n        self.Head5=extractAttention( input_dim, output_dim )\n        self.Head6=extractAttention( input_dim, output_dim )\n        self.Head7=extractAttention( input_dim, output_dim )\n\n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n        temp4=self.Head4(x)\n        temp5=self.Head5(x)\n        temp6=self.Head6(x)\n        temp7=self.Head7(x)\n\n        temp=(temp0+temp1+temp2+temp3+temp4+temp5+temp6+temp7)/8\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8head,4extract\nclass extractAttention(nn.Module):\n    def __init__(self,  input_dim, output_dim):\n        super().__init__()\n        self.query = nn.Linear(input_dim, output_dim)\n        self.key = nn.Linear(input_dim, output_dim)\n        self.value = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n        \n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n        \n        kweight=self.key.weight\n        \n        attn_weights1 = F.softmax(q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5), dim=-1)\n        Vmix0 = attn_weights1 @ v\n        \n        k1=self.extract(k,kweight)\n        attn_weights1 = F.softmax(q @ k1.transpose(-2, -1) / (k1.shape[-1] ** 0.5), dim=-1)\n        Vmix1 = attn_weights1 @ v\n        \n        k2=self.extract(k1,kweight)\n        attn_weights2 = F.softmax(q @ k2.transpose(-2, -1) / (k2.shape[-1] ** 0.5), dim=-1)\n        Vmix2 = attn_weights2 @ v\n        \n        k3=self.extract(k2,kweight)\n        attn_weights3 = F.softmax(q @ k3.transpose(-2, -1) / (k3.shape[-1] ** 0.5), dim=-1)\n        Vmix3 = attn_weights3 @ v\n        \n        k4=self.extract(k3,kweight)\n        attn_weights4 = F.softmax(q @ k4.transpose(-2, -1) / (k4.shape[-1] ** 0.5), dim=-1)\n        Vmix4 = attn_weights4 @ v\n        \n        Vmix=(Vmix0+Vmix1+Vmix2+Vmix3+Vmix3)/5\n       \n        \n        return Vmix\n    def extract(self,k,kweight):\n        with torch.no_grad():\n            return torch.matmul(torch.matmul(k,kweight),torch.transpose(kweight,0,1))  \n           \n\n\nclass extractMultihead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.Head0=extractAttention( input_dim, output_dim )\n        self.Head1=extractAttention( input_dim, output_dim )\n        self.Head2=extractAttention( input_dim, output_dim )\n        self.Head3=extractAttention( input_dim, output_dim )\n        self.Head4=extractAttention( input_dim, output_dim )\n        self.Head5=extractAttention( input_dim, output_dim )\n        self.Head6=extractAttention( input_dim, output_dim )\n        self.Head7=extractAttention( input_dim, output_dim )\n\n        \n    def forward(self,x):\n        temp0=self.Head0(x)\n        temp1=self.Head1(x)\n        temp2=self.Head2(x)\n        temp3=self.Head3(x)\n        temp4=self.Head4(x)\n        temp5=self.Head5(x)\n        temp6=self.Head6(x)\n        temp7=self.Head7(x)\n\n      \n        temp=(temp0+temp1+temp2+temp3+temp4+temp5+temp6+temp7)/8\n        \n        return temp\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, in_channels=3, patch_size=8, out_channels=32):\n        super().__init__()\n        self.patch_size = patch_size\n        self.projection = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=patch_size, stride=patch_size),\n            nn.Flatten(2),\n            \n        )\n\n    def forward(self, x):\n       \n        x = self.projection(x)\n        \n        x=torch.transpose(x,1, 2)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class resBlock(nn.Module):\n    def __init__(self,input_dim,output_dim):\n        super().__init__()\n        self.input_dim=input_dim\n        self.output_dim=output_dim\n        self.resfc=nn.Linear(input_dim,output_dim)\n\n    def forward(self,x):\n        out=torch.relu(x)\n        out_temp=out\n        out=torch.relu(self.resfc(out)+out_temp)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EMH(nn.Module):\n    def __init__(self, in_channels, patch_size, out_channels, img_size,input_dim, output_dim,num_classes):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(in_channels, patch_size, out_channels)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, out_channels))\n        self.positions = nn.Parameter(torch.randn((img_size[0] // patch_size) *(img_size[1] // patch_size) , out_channels))\n        self.transformer = extractMultihead(input_dim, output_dim)\n        self.res1=resBlock(output_dim*out_channels, output_dim*out_channels)\n        self.res3=resBlock(num_classes, num_classes)\n        self.net1 = nn.Sequential(\n            \n            nn.Flatten(1),\n            nn.Linear(output_dim*out_channels, output_dim*out_channels),\n            nn.GELU(),\n            self.res1,\n            nn.Linear(output_dim*out_channels, 2*output_dim*out_channels),\n            nn.Linear(2*output_dim*out_channels, output_dim*out_channels),\n            nn.Linear(output_dim*out_channels, num_classes)\n            \n        )\n        self.mlp_head = nn.Sequential(\n            nn.Linear(num_classes, num_classes),\n            self.res3,\n            nn.GELU(),\n            nn.Linear(num_classes, num_classes)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        b, _, _, _ = x.shape\n        \n        x = self.patch_embedding(x)\n        \n        x += self.positions\n        \n        x=torch.transpose(x,1, 2)\n        x = self.transformer(x)\n        \n        x = self.net1(x)\n        x = self.mlp_head(x)\n        \n        return x\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height=width=256 \nimg_size=(height,width)#the image dimensions\n\nbatch_size=32 #The dataset is grouped and the number of elements in the group\n\nin_channels,out_channels= 3,64 #The number of input tensor layers and the number of output tensor layers of the feature extraction convolution layer\n\npatch_size = 16 #The image is cut into convolutional layer chunks\n\ninput_dim= (height//patch_size)*(width//patch_size)#How many blocks an image is divided into the extractAttention module, that is, the input dimension of the module\noutput_dim=32 #The output dimension of the Extrakten module\n\nnum_classes= 10 #Classification kinds\n\nepoch=100\ni=0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset = datasets.ImageFolder('/kaggle/input/tomato-disease', transform=transform)\n\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\n\n\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=EMH(in_channels, patch_size, out_channels, img_size,input_dim, output_dim,num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\nimport torch.optim as optim\noptimizer = optim.Adam(model.parameters(), lr=0.0001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval() \nwith torch.no_grad(): \n    correct = 0\n    total = 0\n    for inputs, labels in test_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the model on the validation data: {format(100 * correct / total)}%')\nfor t in range(epoch):\n    i=i+1\n    loss_total=0\n    b=0\n    for inputs, labels in train_dataloader:\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n\n        outputs = model(inputs)\n\n\n        loss = criterion(outputs, labels).to(device)\n        b=b+1\n       \n        loss_total=loss_total+loss\n        \n\n        optimizer.zero_grad()\n        \n        loss.backward()\n        optimizer.step()\n        \n    loss_avg=loss_total/b\n    print(f'loss_avg[{i}]={loss_avg}\\n') \n    if i%10==0:\n        model.eval() \n        with torch.no_grad():  \n            correct = 0\n            total = 0\n            for inputs, labels in test_dataloader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        print(f'epoch[{i}]Accuracy of the model on the validation data: {format(100 * correct / total)}%')\n","metadata":{},"execution_count":null,"outputs":[]}]}